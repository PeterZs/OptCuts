% !TeX root = OptCuts.tex

\section{Optimization Framework}

To solve the saddle-point problem in Eq.~\ref{eq:p2}, we alternately improve both our primal variables $(T, U)$ and our dual variable $\lambda$ starting from an initial UV map $(T^0, U^0)$ and $\lambda^0 = 0$ (Algorithm~\ref{alg:selfWeight}).

\begin{algorithm}[!h]
\SetAlgoLined
\KwData{$M$, $T^0$, $U^0$, $b_d$}
\KwResult{$T^*$, $U^*$}

$\lambda^0 \leftarrow 0$, $k \leftarrow 1$\;

\Do{either primal or dual not converged}{
  $\lambda^{k}$ $\leftarrow$ dualUpdate($T^{k-1}$, $U^{k-1}$, $b_d$, $\lambda^{k-1}$); // Section~\ref{sec:dualUpdate}\\

  $(T^{k}, U^{k})$ $\leftarrow$ primalUpdate($M$, $T^{k-1}$, $U^{k-1}$, $\lambda^{k}$); // Section~\ref{sec:primalUpdate}\\

  $k \leftarrow k + 1$\;
}
$(T^*, U^*)$ $\leftarrow$ $(T^{k-1}, U^{k-1})$\; 

\caption{OptCuts}
\label{alg:selfWeight}
\end{algorithm}

\subsection{Initialization}
To obtain an initial UV map for an input surface, we map its initial seam to a circle preserving edge length and parameterize the rest of the vertices through Tutte embedding with uniform weights.

We compute initial seams for different surfaces according to their topology and geometry. For disk-topology surfaces, we simply pick their longest boundary as the initial seam. For genus-0 closed surfaces, we randomly pick 2 connected edges as the initial seam. \minchen{[TODO] change to curvest one point cut or farthest point cut if they are better} For high-genus surfaces, we follow Crane et al.~\shortcite{Crane:2013:DGP} to detect homology generators and connect all of them as the initial seam \minchen{[TODO]}.

We simply start by ignoring the distortion constraints with $\lambda$ set to $0$, and let our dual update to modify $\lambda$ according to the intermediate distortions.

\subsection{Dual Update}
\label{sec:self_weighting}
\label{sec:dualUpdate}

% Our overall minimization is inequality constrained with a specified upper bound $b \in \mathbb{R}_+$ on distortion. \justin{I moved a parenthetical to a Minchen comment assuming he'll write it more formally}\minchen{(L2 norm on SD  energy for now - pretty easy to modify to an extremal measure if we want later on.)}

%
In this section we describe the update for the dual variable $\lambda^k$ given the current UV map $(T^{k-1}, U^{k-1})$.

The inner objective\minchen{Danny: which?} in the saddle-point problem (Eq.~\ref{eq:p2}) is nonsmooth in $\lambda$ since it does not take into account the fact that we might start away from feasibility and want to iteratively improve both our primal variables $(T, U)$ and our dual variable $\lambda$. To smoothly update to a current $\lambda^{k}$ in iteration $k$ from a previous estimate $\lambda^{k-1}$, we add a simple quadratic regularizer $R(\lambda,\lambda^{k-1}) = \frac{1}{2\kappa} (\lambda- \lambda^{k-1})^2$ to make sure $\lambda$ iterates behave reasonably. (We simply set $\kappa = 1$.)

For iteration $k$ this gives us 
\[ \min_{T,V} \max_{\lambda \geq 0} E_{s}(T^{k-1}) + \lambda \big( E_{d}(T^{k-1}, U^{k-1}) - b_d\big) - \frac{1}{2\kappa} (\lambda- \lambda^{k-1})^2 \]
which can be solved in closed form as
\[ \lambda^{k} \leftarrow \max\big(0,\kappa \big( E_{d}(T^{k-1}, U^{k-1}) -b \big) + \lambda^{k-1}\big) \]

\danny{(Notice that throughout the above we can define a progressive $\lambda$ without needing to employ subgradients to reason about nonsmoothness in our sparsity energy.)}\justin{didn't follow this, not sure it's needed}

\justin{Can you call the lambda update a proximal step?}


\subsection{Primal Update}
\label{sec:primalUpdate}

Our $(k+1)$-th primal update is a joint discrete-continuous search step towards minimizing \eqref{eq:L} for a fixed $\lambda^{k+1}$ starting from $(T^k, U^k)$.
Rather than trying to approximate $T$ with non-smooth energies of $U$ and solve a potentially ill-conditioned real-valued optimization, we alternate local optimizations over $U$ (in smooth descent steps) and then $T$ (in topology descent steps) in each inner iteration and obtain $(T^{k+1}, U^{k+1})$ when both steps are converged (Algorithm~\ref{alg:DCSearch}).

\begin{algorithm}[h]
\SetAlgoLined
\KwData{$M$, $T^{k}$, $U^{k}$, $\lambda^{k+1}$}
\KwResult{$T^{k+1}$, $U^{k+1}$}
$i \leftarrow 1$, $T^{k,0} \leftarrow T^{k}$, $U^{k,0} \leftarrow U^{k}$\;
$\delta^{k,0} \leftarrow 0$\;
\Do{either step is not converged}
{
	
	($T^{k,i}$, $U_a^{k,i-1}$) $\leftarrow$ topologyDescentStep($M$, $T^{k,i-1}$, $U^{k,i-1}$, $\delta^{k,i-1}$, $\lambda^{k+1}$); // Section~\ref{sec:topologyStep}\\
	($U^{k,i}$, $\delta^{k,i}$) $\leftarrow$ smoothDescentStep($M$, $T^{k,i}$, $U_a^{k,i-1}$); // Section~\ref{sec:descentStep}\\
	$i \leftarrow i+1$\;
}
$T^{k+1} \leftarrow T^{k,i-1}$, $U^{k+1} \leftarrow U^{k,i-1}$
\caption{Primal Update $k+1$}
\label{alg:DCSearch}
\end{algorithm}

The continuous search is peformed in each smooth descent step, where we conduct a complete Newton-type iteration with backtracking line search towards minimizing distortion $E_d$ over vertices $U$ while holding topology, $T$, fixed (Section~\ref{sec:descentStep}).

Similarly, the discrete topology search is also composed of two stages: First decide a search direction (initiating a local seam edit), and then compute a proper step size (extending the edit) to ensure sufficient energy decrease. However, since each of our topology descent step searches only the neighboring topology to maximize a first-order reduction in the Lagrangian $L$, we interleave topology descent steps with smooth descent steps to perform a forward line search in topology space - their step size is \emph{increased}, rather than backtracked (Section~\ref{sec:topologyStep}).

The role of each topology descent step, either computing search direction or extending the step size, is controlled by a candidate neighbor set to search from. By thresholding the topology descent steps using the energy decrease $\delta^{k,i}$ from the current smooth descent step, we always perform the descent step that decrease the multi-objective more.\minchen{add forward references}

In total, by ensuring monotonic decrease in $L$ over both the smooth and topology descent steps, we then prove that a near-stationary point can be reached for any input within a bounded number of alternations (Section~\ref{sec:convergence}). \danny{for varying $\lambda$ - i.e. our constrained problem seems like we need to spend a bit more work on this - see comments below.}

% \vova{The section below seem to be concerned with convergence of Alg 1, which is beyond self-weighted objective, IMHO... 
% It is also confusing that there a subsection 5.7 on convergence...}
\subsection{Convergence}

Before and after each primal update, our continous search is under convergence. If at the first inner iteration of a primal update there is no neighboring topology that could reduce the multi-objective in the first-order manner, we say our discrete topology search also converges, so does the primal update, and the UV map will be the minimizer of the multi-objective $L$.

Theoretically, the dual update converges in two situations: either at $\lambda^* = 0$ when the primal minimizer $(T^*, U^*)$ is inside the feasible region ($E_d(T^*, U^*) < b_d$) with no seams, or at some $\lambda^* > 0$ with $(T^*, U^*)$ on the boundary of the feasible region ($E_d(T^*, U^*) = b_d$) \cite{a computational optimization book}. However, for the letter case, since there is only a finite number of configurations of $T$, the near stationary $E_d$'s may not be exactly equal to $b_d$. Instead of setting a relatively large tolerance for detecting convergence, we stop when it first tries to violate distortion bound $b_d$ from being feasible.
