% !TeX root = OptCuts.tex

\section{Self-Weighted Objective}
\label{sec:self_weighting}

% Our overall minimization is inequality constrained with a specified upper bound $b \in \mathbb{R}_+$ on distortion. \justin{I moved a parenthetical to a Minchen comment assuming he'll write it more formally}\minchen{(L2 norm on SD  energy for now - pretty easy to modify to an extremal measure if we want later on.)}

The inner objective\minchen{Danny: which?} in the saddle-point problem (Eq.~\ref{eq:p2}) is nonsmooth in $\lambda$ since it does not take into account the fact that we might start away from feasibility and want to iteratively improve both our primal variables $(T, U)$ and our dual variable $\lambda$ (Algorithm~\ref{alg:selfWeight}).

\begin{algorithm}[!h]
\SetAlgoLined
\KwData{$M$, $T^0$, $U^0$, $b_d$}
\KwResult{$T^*$, $U^*$}

$\lambda^0 \leftarrow 0$, $k \leftarrow 0$\;

\Do{not converged}{
  $k \leftarrow k + 1$\;

  $\lambda^{k} \leftarrow \max\big(0,\kappa(E_{d}(T^{k-1}, U^{k-1}) - b_d) + \lambda^{k-1} \big)$\; 

  $(T^k, U^k)$ $\leftarrow$ primalUpdate($T^{k-1}$, $U^{k-1}$, $\lambda^{k}$)\;
}
$(T^*, U^*)$ $\leftarrow$ $(T^k, U^k)$\; 

\caption{Self-Weighting}
\label{alg:selfWeight}
\end{algorithm}

To smoothly update to a current $\lambda^{k}$ in iteration $k$ from a previous estimate $\lambda^{k-1}$, we add a simple quadratic regularizer $R(\lambda,\lambda^{k-1}) = \frac{1}{2\kappa} (\lambda- \lambda^{k-1})^2$ to make sure $\lambda$ iterates behave reasonably.

For iteration $k$ this gives us 
\[ \min_{T,V} \max_{\lambda \geq 0} E_{s}(T^{k-1}) + \lambda \big( E_{d}(T^{k-1}, U^{k-1}) - b_d\big) - \frac{1}{2\kappa} (\lambda- \lambda^{k-1})^2 \]

And we can first solve closed form for $\lambda$ as 
\[ \lambda^{k} \leftarrow argmax_{\lambda \geq 0} E_{s}(T^{k-1}) + \lambda \big( E_{d}(T^{k-1}, U^{k-1}) - b_d\big) - \frac{1}{2\kappa} (\lambda- \lambda^{k-1})^2 \]
giving us 
\[ \lambda^{k} \leftarrow \max\big(0,\kappa \big( E_{d}(T^{k-1}, U^{k-1}) -b \big) + \lambda^{k-1}\big) \]
We then can update the primal variables towards solving
\[ \min_{T,V}  E_{s}(T) + \lambda^{k}  E_{d}(T, U) \]
(Section~\ref{sec:DCSearch}), followed by the next update of dual variable $\lambda$.

\danny{(Notice that throughout the above we can define a progressive $\lambda$ without needing to employ subgradients to reason about nonsmoothness in our sparsity energy.)}\justin{didn't follow this, not sure it's needed}

\justin{Can you call the lambda update a proximal step?}

\paragraph{Convergence}
Theoretically, the primal minimizer $(T^*, U^*)$ will be either inside the feasible region ($E_d(T^*, U^*) < b_d$) with $\lambda^* = 0$ or on the boundary of the feasible region ($E_d(T^*, U^*) = b_d$) with $\lambda^* > 0$ \cite{a computational optimization book}. For the former case, the minimum of $E_s$, which is $0$, will be reached, and this is only true for a small set of nearly developable disk-topology surfaces. We thus look into the latter case that is more common and interesting here.

By formulation, our algorithm converges when $L$ is near-stationary w.r.t. both the primal and dual variables. That is, $E_d(T^*, U^*) = b_d$ and $(T^*, U^*)$ is the local minimizer of the multi-objective $L(T,U,\lambda^*)$. However, since there is only a finite number of configurations of $T$, the near stationary $E_d$'s may not be exactly equal to $b_d$. Instead of setting a relatively large tolerance for detecting convergence, we stop when it first tries to violate distortion bound $b_d$ from being feasible.
