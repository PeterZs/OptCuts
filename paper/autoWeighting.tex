% !TeX root = OptCuts.tex

\section{Self-Weighted Objective}
\label{sec:self_weighting}
\danny{This section gives the ``outer'' loop details - I'd suggest moving it above the sections on the inner-loop details in the paper as it's short and would help readers understand the algorithm structure (roadmap!). Then jump into the inner loop detail sections - this would follow the structure of the actual algorithm.}
\subsection{Formulation}
At each inner iterate $k+1$, we fix some $\lambda^{k+1}$ and minimize the bi-objective 
\[ \min_{T,V} E_{SE}(V,T) + \lambda^{k+1} E_{SD}(V,T) \]

How do we get $\lambda^{k+1}$? \justin{not sure a rhetorical question is needed}

Our overall minimization is inequality constrained with a specified upper bound $b \in \mathbb{R}_+$ on distortion. \justin{I moved a parenthetical to a Minchen comment assuming he'll write it more formally}\minchen{(L2 norm on SD  energy for now - pretty easy to modify to an extremal measure if we want later on.)}

Our model problem minimization is then 
\[ \min_{T,V} E_{SE}(V,T) :  b - E_{SD}(V,T) \geq 0 \]

Or, equivalently,
\[ \min_{T,V} \max_{\lambda \geq 0} E_{SE}(V,T) + \lambda \big( E_{SD}(V,T) - b\big) \]
\justin{the two equations above are repeats from sec3; remove them and just use eq numbers.}

The inner objective is nonsmooth in $\lambda$ since it does not take into account %very nicely 
the fact that per-iteration we will start away from feasibility and want to iteratively improve both our primal variables $\{V,T\}$ and our dual variable $\lambda$. \justin{I didn't follow the previous sentence} To smoothly update to a current $\lambda^{k+1}$ from a previous estimate $\lambda^k$ we will add a regularizer $R(\lambda,\lambda^k)$ to make sure $\lambda$ iterates behave reasonably. For now lets stick with something simple:  a quadratic regularizer should do the trick  $R =\frac{1}{2\kappa} (\lambda- \lambda^k)^2$.  \justin{Previous sentence is too informal, and not sure ``for now'' is right}

For iteration $k+1$ this gives us 
\[ \min_{T,V} \max_{\lambda \geq 0} E_{SE}(V,T) + \lambda \big( E_{SD}(V,T) - b\big) - \frac{1}{2\kappa} (\lambda- \lambda^k)^2 \]

And now we can first solve closed form for $\lambda$ as 
\[ \lambda^{k+1} \leftarrow argmax_{\lambda \geq 0} E_{SE}(V,T) + \lambda \big( E_{SD}(V,T) - b\big) - \frac{1}{2\kappa} (\lambda- \lambda^k)^2 \]
giving us 
\[ \lambda^{k+1} \leftarrow \max\big(0,\kappa \big( E_{SD}(V,T) -b \big) + \lambda^k\big) \]
We then can solve the inner iteration (with both discrete topology steps and smooth steps) with the energy 
\[ \min_{T,V}  E_{SE}(V,T) + \lambda^{k+1}  E_{SD}(V,T), \]
followed by the next update of dual variable $\lambda$.

(Notice that throughout the above we can define a progressive $\lambda$ without needing to employ subgradients to reason about nonsmoothness in our sparsity energy.)\justin{didn't follow this, not sure it's needed}

\justin{Might be worth adding a few sentences referencing that this is an instance of previous optimization techniques.  Can you call the lambda update a proximal step?}

\subsection{Implementation}

\begin{algorithm}[!h]
\SetAlgoLined
\KwData{Input model with initial UV map, expected $E_{SD}$ upper bound $b$}
\KwResult{UV map with $argmin_{v_T} E_{se}$ topology and $E_{SD} <= b$}

$\lambda \leftarrow +\infty$ \\
\For{each alternating iteration $i$}{
  descent step $i$\;
  \If{$E^i_{SD} <= b$}{
  	$j \leftarrow i$
    break;
  }
  $i \leftarrow i + 1$\;
  topology step $i$\;
}

$\lambda \leftarrow (E^j_{se} - E^{j-1}_{se})/(E^{j-1}_{SD}-E^{j}_{SD})$\;

\For{each alternating iteration $i$}{
  topology step\;
  descent step\;

  \If{$(E^i_{se}, E^i_{SD}, \lambda)$ has occurred before}{
    set to best feasible UV map and break\;
  }
  \If{at $\min_{V,T} E_{se} + \lambda E_{SD}$}{
    \If{$E^i_{SD} <= b$  $E^i_{SD} >= b - tol$}{
      set to best feasible UV map and break\;
    }
  }
  
  $\lambda \leftarrow \max\big(0,\kappa(E_{SD}(V,T) -b) + \lambda \big)$\;
  \If{at $\min_{V,T} E_{se} + \lambda E_{SD}$}{
    \If{picking the wrong type of operation}{
      do
        $\lambda \leftarrow \max\big(0,\kappa(E_{SD}(V,T) -b) + \lambda \big)$\;
      while picking the wrong type of operation
    }
    \Else {
      while candidate selection didn't change
        $\lambda \leftarrow \max\big(0,\kappa(E_{SD}(V,T) -b) + \lambda \big)$\;
      end
    }
  }
}

\caption{Self-Weighted $E_w$}
\end{algorithm}

\minchen{[NOTE] To understand alternating lambda updates between PN iterations: lambda update changes the energy manifold, so it couldn't be too frequent or the manifold can't really be explored enough, nor it could be too seldom or it would be trapped in a local region.}