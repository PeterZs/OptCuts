% !TeX root = OptCuts.tex

\section{Self-Weighted Objective}
\label{sec:self_weighting}

\subsection{Formulation}
At each inner iterate $k+1$, we fix some $\lambda^{k+1}$ and minimize the bi-objective 
\[ \min_{T,V} E_{SE}(V,T) + \lambda^{k+1} E_{SD}(V,T) \]

How do we get $\lambda^{k+1}$? 

Our overall minimization is inequality constrained with a specified upper bound $b \in \mathbb{R}_+$ on distortion. (L2 norm on SD  energy for now - pretty easy to modify to an extremal measure if we want later on.)

Our model problem minimization is then 
\[ \min_{T,V} E_{SE}(V,T) :  b - E_{SD}(V,T) \geq 0 \]

Or, equivalently,
\[ \min_{T,V} \max_{\lambda \geq 0} E_{SE}(V,T) + \lambda \big( E_{SD}(V,T) - b\big) \]

Of course this is nonsmooth in $\lambda$ since it does not take into account very nicely the fact that per-iteration we will start away from feasibility and want to iteratively improve both our primal variables $\{V,T\}$ and our dual variable $\lambda$.  So to smoothly update to a current $\lambda^{k+1}$ from a previous estimate $\lambda^k$ we will add a regularizer $R(\lambda,\lambda^k)$ to make sure $\lambda$ iterates behave themselves reasonably. For now lets stick with something simple: a quadratic regularizer should do the trick  $R =\frac{1}{2\kappa} (\lambda- \lambda^k)^2$. 

For iteration $k+1$ this gives us 
\[ \min_{T,V} \max_{\lambda \geq 0} E_{SE}(V,T) + \lambda \big( E_{SD}(V,T) - b\big) - \frac{1}{2\kappa} (\lambda- \lambda^k)^2 \]

And now we can first solve closed form for $\lambda$ as 
\[ \lambda^{k+1} \leftarrow argmax_{\lambda \geq 0} E_{SE}(V,T) + \lambda \big( E_{SD}(V,T) - b\big) - \frac{1}{2\kappa} (\lambda- \lambda^k)^2 \]

giving us 
\[ \lambda^{k+1} \leftarrow \max\big(0,\kappa \big( E_{SD}(V,T) -b \big) + \lambda^k\big) \]

We then can solve the inner iteration (with both discrete topology steps and smooth steps) with the energy 
\[ \min_{T,V}  E_{SE}(V,T) + \lambda^{k+1}  E_{SD}(V,T) \]

Followed by the next update of dual variable $\lambda$.

(Notice that throughout the above we can define a progressive $\lambda$ without needing to employ subgradients to reason about nonsmoothness in our sparsity energy.)

\subsection{Implementation}

\begin{algorithm}[!h]
\SetAlgoLined
\KwData{Input model with initial UV map, expected $E_{SD}$ upper bound $b$}
\KwResult{UV map with $argmin_{v_T} E_{se}$ topology and $E_{SD} <= b$}

$\lambda \leftarrow +\infty$ \\
\For{each alternating iteration $i$}{
  descent step $i$\;
  \If{$E^i_{SD} <= b$}{
  	$j \leftarrow i$
    break;
  }
  $i \leftarrow i + 1$\;
  topology step $i$\;
}

$\lambda \leftarrow (E^j_{se} - E^{j-1}_{se})/(E^{j-1}_{SD}-E^{j}_{SD})$\;

\For{each alternating iteration $i$}{
  topology step\;
  descent step\;

  \If{$(E^i_{se}, E^i_{SD}, \lambda)$ has occurred before}{
    set to best feasible UV map and break\;
  }
  \If{at $\min_{V,T} E_{se} + \lambda E_{SD}$}{
    \If{$E^i_{SD} <= b$  $E^i_{SD} >= b - tol$}{
      set to best feasible UV map and break\;
    }
  }
  
  $\lambda \leftarrow \max\big(0,\kappa(E_{SD}(V,T) -b) + \lambda \big)$\;
  \If{at $\min_{V,T} E_{se} + \lambda E_{SD}$}{
    \If{picking the wrong type of operation}{
      do
        $\lambda \leftarrow \max\big(0,\kappa(E_{SD}(V,T) -b) + \lambda \big)$\;
      while picking the wrong type of operation
    }
    \Else {
      while candidate selection didn't change
        $\lambda \leftarrow \max\big(0,\kappa(E_{SD}(V,T) -b) + \lambda \big)$\;
      end
    }
  }
}

\caption{Self-Weighted $E_w$}
\end{algorithm}